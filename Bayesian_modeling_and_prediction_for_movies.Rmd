---
title: "Bayesian modeling and prediction for movies"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    theme: spacelab
---



Rotten Tomatoes and IMDB are the two most trusted source to measure the performance and quality of movie. Rotten Tomatoes introduced one model as Tomatometer as trusted measurement of the quality of the entertainment. This is based on user provided rating, reviews of millions of users on different TV shows, feature flims and TV movies. Rotten Tomatoes designates the best reviewed movies and TV shows as Certified Fresh. That accolade is awarded with Tomatometer ratings of 75% and higher, and a required minimum number of reviews.

<h3>What is the Tomatometer?</h3>

The Tomatometer rating - based on the published opinions of hundreds of film and television critics - is a trusted measurement of movie and TV programming quality for millions of moviegoers. The Tomatometer rating represents the percentage of professional critic reviews that are positive for a given film or television show. Back in the days of the open theaters, when a play was particularly atrocious, the audience expressed their dissatisfaction by not only booing and hissing at the stage, but throwing whatever was at hand -- vegetables and fruits included.

->The full popcorn bucket means the movie received 3.5 stars or higher by Flixster and Rotten Tomatoes users.
->The tipped over popcorn bucket means the movie received less than 3.5 stars by Flixster and Rotten Tomatoes users.
->The plus sign will appear for movies that do not have audience ratings or reviews. The percentage you see associated with this icon is the percentage of users who added the movie to their Want-to-See list.

<h3>What is the audience score?</h3>
The Audience rating, denoted by a popcorn bucket, is the percentage of all Flixster.com and RottenTomatoes.com users who have rated the movie or TV Show positively.

->A good review is denoted by a fresh red tomato. In order for a movie or TV show to receive an overall rating of Fresh, the reading on the Tomatometer for that movie must be at least 60%.


->A bad review is denoted by a rotten green tomato splat (59% or less).

->To receive a Certified Fresh rating a movie must have a steady Tomatometer rating of 75% or better. Movies opening in wide release need at least 80 reviews from Tomatometer Critics (including 5 Top Critics). Movies opening in limited release need at least 40 reviews from Tomatometer Critics (including 5 Top Critics). A TV show must have a Tomatometer Score of 75% or better with 20 or more reviews from Tomatometer Critics (including 5 Top Critics). If the Tomatometer score drops below 70%, then the movie or TV show loses its Certified Fresh status. In some cases, the Certified Fresh designation may be held at the discretion of the Rotten Tomatoes editorial team.

<h3>Data Collection</h3>

Main source of these data is crowd sourcing. Though it started as a hobby project but now it became the most trusted source of celebrity content, authoritative source for movie and TV shows.

Though they are mostly dependend  on crowd sourcing but their content gets feed from different news feeds and data sources. But they they have a strong content review model and technical facilities to valided those contents and focus on reliable sources.

<h5>Data Source and further information link</h5>

Dataset: [Data Set] https://d3c33hcgiwev3.cloudfront.net/_e1fe0c85abec6f73c72d73926884eaca_movies.Rdata?Expires=1502150400&Signature=bRT93uon8xdCRVyQoTb2jNM1qjl7nGY4Tqr5BMAv57V4B1xKAAVZsJrQlXADbzZhBcFJuxwz2cixNHvlKHpXV28CSBbMDv5gTRTXakMfh1d5DsAr~PoW~AKh6R4kp3POdCWmJSGvIm-Er8dQtWc5LJk61oPQvCKVWi-RBb6vWBs_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A

Codebook: [Movies Codebook](https://d3c33hcgiwev3.cloudfront.net/_73393031e98b997cf2445132f89606a1_movies_codebook.html?Expires=1473465600&Signature=M9ShITsr7TZCndmtN5udxcVWqXa-kYwLJrhFhD63eXcIAG4cpVnpHKSJCHi7kaNCQ-TPSpR4HPPL~zUJoVR1ZlUBS6jSLOCuRHTjcLoWueb40h2LF9wWh12d4ZFSggpHhY3GFEiXvrr1aDANMFHMuAuCJ1BdYxRlE-FViJesVkI_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)

<h5>source information </h5>
Rotten Tomatoes: [Rotten Tomatoes](https://www.rottentomatoes.com/)

IMDB: [IMDB](http://www.imdb.com/)

Generalizability

The present data were derived from an observational study. The data set is comprised of 651 randomly sampled movies produced and released from 1970 to 2014. According to IMDb, there have 9,962 movies been release from 1972 to 2016 so that the 10% condition (9,962*10% = 996) is met. Since the sampling size is large enough and less than 10% of population, it can assume that the random sampling is conducted. Therefore we can conclude that the sample is indeed generalizable to the entire population.
Causality

The data cannot be used to establish a causal relation between the variables of interest as there was no random assignment to the explanatory and independent variables.

## Setup

### Load packages

```{r load-packages, message = FALSE}
library(ggplot2)
library(dplyr)
library(statsr)
library(BAS)
library("tidyr")
library('GGally')
library('knitr') 
library('grid')
library('gridExtra')
library('MASS')
source('Utilities.R')

# apply general knitr options
knitr::opts_chunk$set(comment=NA, fig.align='center')
```

### Load data


```{r load-data}
#load("movies.Rdata")
load("imdbdata.gz")
data <- movies

```




* * *

## Part 1: Data

At starting we will perform a intial check on the dimensions of the dataset and the properties of the data.

```{r data-dimension}

dim(data)

```




```{r data-summary}

str(data)
summary(data)

```

## Part 2: Data manipulation

<h3>Part2 : Data Manipulation</h3>

The movies data from IMDB was used for the analysis at hand. Some of variables are in the original dataset provided, and others are new variables. This will need be to construct in the data manipulation section.
feature_film: 'yes' if title_type is Feature Film, 'no' otherwise
drama: "yes" if genre is Drama, "no" otherwise
mpaa_rating_R: "yes" if mpaa_rating is R, "no" otherwise
runtime:
thtr_rel_year:
oscar_season: "yes" if movie is released in November, October, or December (based on thtr_rel_month), "no" otherwise
summer_season: "yes" if movie is released in May, June, July, or August (based on thtr_rel_month), "no" otherwise
imdb_rating:
imdb_num_votes:
critics_score:
best_pic_nom:
best_pic_win:
best_actor_win:
best_actress_win:
best_dir_win:
top200_box:


* * *

Recommended features has been derived in the following code snippet.


```{r data-manipulation}

# reasign yes or no to feature oscar_season, where release month is either  "10", "11", "12"
data <- data %>% mutate(oscar_season = as.factor(ifelse(thtr_rel_month %in% c('10', '11', '12'), 'yes', 'no')))

# reasign yes or no to feature summer_season, where release month is either  "6", "7", "8"
data <- data %>% mutate(summer_season = as.factor(ifelse(thtr_rel_month %in% c('6', '7', '8'), 'yes', 'no')))

# reasign yes or no to feature mpaa_rating_R, where mpaa_rating is "R"
data <- data %>% mutate(mpaa_rating_R = as.factor(ifelse(mpaa_rating == 'R', 'yes', 'no')))

# reasign yes or no to feature drama, where genre is "drama"
data <- data %>% mutate(drama = as.factor(ifelse(genre ==  'Drama', 'yes', 'no')))

# reasign yes or no to feature feature_film, where title_type is "Feature Film"
data <- data %>% mutate(feature_film = as.factor(ifelse(title_type ==  'Feature Film', 'yes', 'no')))


dim(data)
```


Do current dataset contains 37 coloumns where we have 5 derived features to make our model robust and strong.

* * *

## Part 3: Exploratory data analysis

NOTE: Insert code chunks as needed by clicking on the "Insert a new code chunk" 
button above. Make sure that your code is visible in the project you submit. 
Delete this note when before you submit your work.


```{r data-eda}
  features <- c('audience_score', 'oscar_season', 'summer_season', 'mpaa_rating_R', 'drama', 'feature_film' )
  
  # Create a new dataset to explicitely show dataset containing NA's
  data.analysis <- data[, features]
  
  # remove NA's
  data.analysis <- data.analysis[complete.cases(data.analysis), ]

  # calculate the mode of the distribution
  mode <- dmode(data.analysis$audience_score)
  
  ggplot(data = data.analysis, aes(x = audience_score, y = ..density..)) +
  geom_histogram(bins = 40, fill = 'steelblue', colour = 'black') + #bdbdbd
  geom_density(size = 1, colour = 'brown') + #cccccc
  geom_vline(data = data, mapping = aes( xintercept = mean(data$audience_score),
             colour = 'steelblue', show_guide = F ), size = 1.5) +
  geom_vline(data = data,mapping = aes( xintercept = median(data$audience_score),
             colour = 'green', show_guide = F), size = 1.5) +
  geom_vline(data = data, mapping = aes( xintercept = mode, colour = 'red', show_guide = F), 
             size = 1.5) +
  geom_text(data = data, aes( x = (mean(data$audience_score) - 5), y = .020, label = 'mean',
            colour = 'steelblue'), size = 4, parse = T) +
  geom_text(data = data,aes( x = (median(data$audience_score) + 5),y = .020,  label = 'median',
            colour = 'green'), size = 4, parse = T) +
  geom_text(data = data, aes( x = (mode + 5), y = .020, label = 'mode', colour = 'red'),
            size = 4, parse = T)

  mean(data.analysis$audience_score)
  median(data.analysis$audience_score)
  mode
  
  summary(data.analysis$audience_score)
  IQR(data.analysis$audience_score)

```


These plots represents analysis and relationship of the audience score with other features. 


<h2>Analysis of the newly created features</h2>

These part represents analysis of the 5 derived features from the dataset.We'll have a look at the newly created features oscar_season, summer_season, mpaa_rating_R, drama and feature_film. We create a boxplot for each feature, comparing them with the audience_score. We will also analyse the variability of the new features by comparing them to each other.


```{r data-eda-newfeatures}
data.grouped <- gather(data.analysis, 'features', 'flag', 2:6)

p1 <- ggplot(data = data.analysis, aes(x = summer_season, y = audience_score, fill = summer_season)) + 
      geom_boxplot() + ggtitle('Audience score vs summer season') + xlab('summer season') + 
      ylab('Audience Score') + scale_fill_brewer(name = "summer season")

p2 <- ggplot(data = data.analysis, aes(x = oscar_season, y = audience_score, fill = oscar_season)) + 
      geom_boxplot() + ggtitle('Audience score vs oscar_season') + xlab('oscar_season') + 
      ylab('Audience Score') + scale_fill_brewer(name = "oscar_season")

p3 <- ggplot(data = data.analysis, aes(x = drama, y = audience_score, fill = drama)) + geom_boxplot() +
      ggtitle('Audience score vs drama') + xlab('drama') + ylab('Audience Score') + 
      scale_fill_brewer(name = "drama")

p4 <- ggplot(data = data.analysis, aes(x = feature_film, y = audience_score, fill = feature_film)) + 
      geom_boxplot() + ggtitle('Audience score vs feature_film') + xlab('feature_film') + 
      ylab('Audience Score') + scale_fill_brewer(name = "feature_film")

p5 <- ggplot(data = data.analysis, aes(x = mpaa_rating_R, y = audience_score, fill = mpaa_rating_R)) + 
      geom_boxplot() + ggtitle('Audience score vs mpaa_rating_R') + xlab('mpaa_rating_R') + 
      ylab('Audience Score') + scale_fill_brewer(name = "mpaa_rating_R")

# arrange the previously created plots 
grid.arrange(p1, p2, p3, p4, p5, ncol = 2)

```




```{r data-eda-newly}
 ggplot(data = data.grouped, aes(x = features, y = audience_score, fill = flag)) + geom_boxplot() +
      ggtitle('Audience score vs grouped featues') + xlab('grouped featues') + ylab('Audience Score') +
      theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
      scale_fill_brewer(name = "grouped featues")

```



```{r data-eda-newly-group}
data.grouped %>%
 group_by(features, flag) %>%
 summarise(mean = mean(audience_score), median = median(audience_score), min = min(audience_score), 
           max = max(audience_score), IQR = IQR(audience_score))
```

The new features do not show much variability in the different features, which leads to the conclusion that none of the above features are valuable towards the prediction of the audience score. The only feature which might be relevant, could be the 'feature_film' feature. There seems to be a clear differentiation. But we shouldn't jump into conclusions right away, therefore we will continue analysing the data.

Finally we will have a look at the other features used in the dataset. I left out the features as runtime, imdb_rating, imdb_num_votes, Above a selection of Density plots have been visualized. The plots show different features compared to the audience score.


```{r data-eda-featurecomparison}
features <- c( 'audience_score', 'feature_film', 'drama', 'runtime', 'mpaa_rating_R', 'thtr_rel_year', 
               'oscar_season', 'summer_season', 'imdb_rating', 'imdb_num_votes', 'critics_score', 
               'best_pic_nom', 'best_pic_win', 'best_actor_win', 'best_actress_win', 'best_dir_win', 
               'top200_box')

    # Create a new dataset to explicitely show dataset containing NA's
    data.model <- data[, features]
    
    # remove NA's
    data.model<- data.model[complete.cases(data.model), ]

    p1 <- ggplot(data.model, aes(audience_score, fill = feature_film))
    p1 <- p1 + geom_density(size=1, colour="darkgreen") + labs(title = "Dist. of audience score vs. feature_film") + 
               labs(x = "feature film", y = "Density")
    
    p2 <- ggplot(data.model, aes(audience_score, fill = drama))
    p2 <- p2 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. drama") + 
               labs(x = "drama", y = "Density")
    
    p3 <- ggplot(data.model, aes(audience_score, fill = top200_box))
    p3 <- p3 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. top200_box") +
               labs(x = "top200 box", y = "Density")
    
    p4 <- ggplot(data.model, aes(audience_score, fill = oscar_season))
    p4 <- p4 + geom_density(size=1, colour="darkgreen") + labs(title = "Dist. of audience score vs. oscar_season") + 
               labs(x = "oscar season", y = "Density")

    p5 <- ggplot(data.model, aes(audience_score, fill = summer_season))
    p5 <- p5 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. summer_season") + 
               labs(x = "summer season", y = "Density")
    
    p6 <- ggplot(data.model, aes(audience_score, fill = best_pic_nom))
    p6  <- p6 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. best_pic_nom") + 
                labs(x = "best pic nom", y = "Density")
    
    p7 <- ggplot(data.model, aes(audience_score, fill = best_pic_win))
    p7 <- p7 + geom_density(size=1, colour="darkgreen") + labs(title = "Dist. of audience score vs. best pic win") + 
               labs(x = "best pic win", y = "Density")
    
    p8 <- ggplot(data.model, aes(audience_score, fill = best_actor_win))
    p8 <- p8 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. best_actor_win") + 
               labs(x = "best actor win", y = "Density")
    
    p9 <- ggplot(data.model, aes(audience_score, fill = best_dir_win))
    p9 <- p9 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. best_dir_win") + 
               labs(x = "best dir win", y = "Density")
    
    p10 <- ggplot(data.model, aes(audience_score, fill = best_actress_win))
    p10 <- p10 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. best_actress_win") + 
                 labs(x = "best actress win", y = "Density")
    
    grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, ncol = 2)
    
```


<h2>Hypothesis Testing </h2>


As with the frequentist approach we use these data to perform basic inference on ?? the audience_score of all movies in the dataset. To do this we will use the bayes_inference function, which will allow us to construct credible intervals perform a hypothesis test and calculate Bayes factors for a variety of different circumstances. The main goal is to investigate if the newly created features influence the audience_score. Remember we assumed that the new fetures will not influence the audience_score very much.

We will conduct a Bayesian hypothesis test by calculating a Bayes factor for each feature. The following hypothesis is meant to be generic and needs to be adjusted for each individual feature (feature_film, drama, mpaa_rating_R, oscar_season and summer_season). We assigned a uniform prior and use a 'two sided' direction for each analysis. I'll just print on of the summaries to indicate the information which is contained. All others will be suppressed to save some space. Enable the params - show_res and show_summ by setting the value to TRUE.

```{r data-eda-hypothesis-testing}
bayes_inference(y = audience_score, data = data.analysis, cred_level = 0.95, statistic = "mean", type = "ci", null = 0, show_res = FALSE, show_summ = FALSE)

bayes_inference(x = feature_film, y = audience_score, data = data.analysis, cred_level = 0.95, hypothesis_prior = c(0.5, 0.5), statistic = "mean", type = "ht", null = 0, alternative = 'twosided', show_res = TRUE, show_summ = TRUE)
bayes_inference(x = feature_film, y = audience_score, data = data.analysis, cred_level = 0.95, hypothesis_prior = c(0.5, 0.5), statistic = "mean", type = "ht", null = 0, alternative = 'twosided', show_res = FALSE, show_summ = FALSE )
bayes_inference(x = oscar_season, y = audience_score, data = data.analysis, cred_level = 0.95, hypothesis_prior = c(0.5, 0.5), statistic = "mean", type = "ht", null = 0, alternative = 'twosided', show_res = FALSE, show_summ = FALSE)
bayes_inference(x = summer_season, y = audience_score, data = data.analysis, cred_level = 0.95, hypothesis_prior = c(0.5, 0.5), statistic = "mean", type = "ht", null = 0, alternative = 'twosided', show_res = FALSE, show_summ = FALSE)
bayes_inference(x = drama, y = audience_score, data = data.analysis, cred_level = 0.95, hypothesis_prior = c(0.5, 0.5), statistic = "mean", type = "ht", null = 0, alternative = 'twosided', show_res = FALSE, show_summ = FALSE)
bayes_inference(x = mpaa_rating_R, y = audience_score, data = data.analysis, cred_level = 0.95, hypothesis_prior = c(0.5, 0.5), statistic = "mean", type = "ht", null = 0, alternative = 'twosided', show_res = FALSE, show_summ = FALSE)

```



# h4 Bayes Factors

|feature |	BF[H1:H2]	| BF[H2:H1]	| Evidence against|
|--------|:----------:|:---------:|----------------:|
|feature_film|		|4.22e+13|	H1 (Very Strong)|
|oscar_season	|13.3993|		|H2 (Positive)|
|summer_season|	19.8084|	|	H2 (Positive)|
|drama|		|22.6567|	H1 (Positive)|
|mpaa_rating_R|	23.9679|		|H2 (Positive)|


* * *
The newly created features seem not to be very indicative for the prediction of the audience_score as shown in the table above. The only feature which shows a differentiation is 'feature_film'. There is strong evidence against H1, which means that there is a significant difference in mean audience_score for feature- and non-feature films. While the data provides a positive evidence that there is no difference between 'audience_scores' and all the other (new) features (mpaa_rating_R, oscar_season, summer_season, drama).

## Part 4: Modeling

The best model is not always the most complicated. Sometimes including variables that are not evidently important, can actually reduce the accuracy of predictions. In practice, the model that includes all available explanatory variables is often referred to as the full model. The full model may not be the best model, and if it isn't, we want to identify a smaller model that is preferable.
Full model:
audience_score ~ feature_film + drama + runtime + mpaa_rating_R + thtr_rel_year + oscar_season + summer_season + imdb_rating + imdb_num_votes + critics_score + best_pic_nom + best_pic_win + best_actor_win + best_actress_win + best_dir_win + top200_box

Bayesian Model Averaging (BMA)
A comprehensive approach to address model uncertainty is Bayesian model averaging, which allows us to assess the robustness of results to alternative specifications by calculating posterior distributions over coefficients and models. Given the 17 features (n) there can be 2^n = 2^17 possible models. We will explore model uncertainty using posterior probabilities of models based on BIC. We will use BIC as a way to approximate the log of the marginal likelihood. The Bayesian information criterion (BIC) runs through 

several fitted model objects for which a log-likelihood value can be obtained, according to the formula -2log-likelihood + nparlog(nobs), where npar represents the number of parameters and nobs the number of observations in the fitted model.

We show the model selection based on two different priors (Zellner and BIC), but we will also compare the model selection based on the following priors "g-prior", "AIC", "hyper-g-n", "EB-local". The last part will indicate the Akaike information criterion (AIC), which uses a backward feature elimination method and the model which is proposed by this approach.



```{r previous-model}
features <- c( 'audience_score', 'feature_film', 'drama', 'runtime', 'mpaa_rating_R', 'thtr_rel_year', 
               'oscar_season', 'summer_season', 'imdb_rating', 'imdb_num_votes', 'critics_score', 
               'best_pic_nom', 'best_pic_win', 'best_actor_win', 'best_actress_win', 'best_dir_win', 
               'top200_box')

    # Create a new dataset to explicitely show dataset containing NA's
    data.model <- data[, features]
    
    # remove NA's
    data.model<- data.model[complete.cases(data.model), ]

    # str(data.model)

    formula <- getFormula(data.model, 'audience_score')
    
    
```

<h2>Bayesian Information Criterion</h2>

```{r model-information-criterion}
audience.BIC = bas.lm(
  formula = formula,
  prior = "BIC",
  modelprior = uniform(),
  data = data.model
)

audience.BIC$probne0
```

<h2>Coefficient Summaries</h2>

The summary outputs have been aggregated for convenience purposes. The section shows the marginal posterior mean, standard deviation and posterior inclusion probabilities obtained by BMA.

```{r Coefficient Summaries}
audience.BIC.coef = coef(audience.BIC)
interval   <- confint(audience.BIC.coef)
names <- c("post mean", "post sd", colnames(interval))
interval   <- cbind(audience.BIC.coef$postmean, audience.BIC.coef$postsd, interval)
colnames(interval) <- names
interval
summary(audience.BIC)
```


<h2>Model Space</h2>

To visualize the space of models (by default the top 20 models in terms of their posterior probabilities), we may use the image function. The image below shows the best model on the left hand side (indicated with a 1). It indicates that the best model can be built by using the features - runtime, critics_score and imdb_rating.
Ideally, of our model assumptions hold, we will not see outliers or non-constant variance. The second plot shows the cumulative probability of the models in the order that they are sampled. This plot indicates that the cumulative probability is levelling off as each additional model adds only a small increment to the cumulative probability, which earlier, there are larger jumps corresponding to sampling high probability models. The third plot shows the dimension of each model (the number of regression coefficients including the intercept) versus the log of the marginal likelihood of the model. The last plot shows the marginal posterior inclusion probabilities (pip) for each of the covariates, with marginal pips greater than 0.5 shown in red. The variables with pip > 0.5 correspond to what is known as the median probability model. Variables with high inclusion probabilities are generally important for explaining the data or prediction, but marginal inclusion probabilities may be small if there are extreme correlations among predictors, like p-values may be large in the presence of multicollinearity.


<h2>Final Model</h2>

Based on the previous analysis we will use the following model to continue with the prediction: audience_score ~ imdb_rating + critics_score
It seems a little fuzzy to me, that a final model basically follows 'one' predictor - 'critics_score'. I suppose there must be better predictors, which we weren't asked to use.

```{r final-modeling-code}
features <- c( 'audience_score', 'imdb_rating', 'critics_score' )

    # Create a new dataset to explicitely show dataset containing NA's
    data.final <- data[, features]
    
    # remove NA's
    data.final<- data.final[complete.cases(data.final), ]
    formula <- getFormula(data.final, 'audience_score')
    
    audience.ZS =  bas.lm(audience_score ~ .,  
                          data = data.final,
                          prior = "ZS-null",
                          modelprior = uniform(),
                          method = "MCMC", 
                          MCMC.iterations = 10^6)    

```


#Inference with model selection

In addition to using BMA, we can use the posterior means under model selection. This corresponds to a decision rule that combines estimation and selection.

#h5 Bayesian model averaging


```{r modeling-code-Bayesian-model-averaging}
model <- audience.ZS
BMA  = predict(model, estimator="BMA")

HPM = predict(model, estimator="HPM")

model.ZS = coef(model)
model.ZS$conditionalmeans[HPM$best,]
model.ZS$conditionalsd[HPM$best,]

BMA <- predict(model, estimator="BMA")  # Bayesian model averaging, using optionally only the 'top' models 
HPM <- predict(model, estimator="HPM")  # The highest probability model 
BPM <- predict(model, estimator="BPM")  # The model that is closest to BMA predictions under squared error loss. 
# MPM <- predict(model, estimator="MPM")  # The median probability model of Barbieri and Berger

library(GGally)
ggpairs(data.frame(HPM = as.vector(HPM$fit),  #this used predict so we need to extract fitted values
                   BPM = as.vector(BPM$fit),  # this used fitted
                   BMA = as.vector(BMA$fit))) # this used predict

BPM = predict(audience.ZS, estimator="BPM", se.fit=TRUE)
audience.conf.fit = confint(BPM, parm="mean")
audience.conf.pred = confint(BPM, parm="pred")
#cbind(audience.conf.fit, audience.conf.pred)
#plot(audience.conf.fit)

model <- audience.ZS
BMA  = predict(model, estimator="BMA")

HPM = predict(model, estimator="HPM")

model.ZS = coef(model)
model.ZS$conditionalmeans[HPM$best,]
model.ZS$conditionalsd[HPM$best,]

BMA <- predict(model, estimator="BMA")  # Bayesian model averaging, using optionally only the 'top' models 
HPM <- predict(model, estimator="HPM")  # The highest probability model 
BPM <- predict(model, estimator="BPM")  # The model that is closest to BMA predictions under squared error loss. 
MPM <- predict(model, estimator="MPM")  # The median probability model of Barbieri and Berger

library(GGally)
ggpairs(data.frame(HPM = as.vector(HPM$fit),  #this used predict so we need to extract fitted values
                   BPM = as.vector(BPM$fit),  # this used fitted
                   BMA = as.vector(BMA$fit))) # this used predict

BPM = predict(audience.ZS, estimator="BPM", se.fit=TRUE)
audience.conf.fit = confint(BPM, parm="mean")
audience.conf.pred = confint(BPM, parm="pred")
cbind(audience.conf.fit, audience.conf.pred)
plot(audience.conf.fit)


```
* * *

## Part 5: Prediction

Finally, I'll provide the predicted value, based on a 95% CI. We used the movie Sully for that purpose. We couldn't find the actual audience score, therefore it is unclear, how precise the predicted values is, but we went the route to train the model with a training set and used the test set to validate the actual values.

```{r modeling-prediction-code}
features <- c( 'audience_score', 'imdb_rating', 'critics_score' )

    # Create a new dataset to explicitely show dataset containing NA's
    data.final <- data[, features]
    
    # remove NA's
    data.final<- data.final[complete.cases(data.final), ]
    
    ds <- splitTrainTestSet(data = data.final, 0.6)
    df.train <- data.frame(ds$train)
    df.test  <- data.frame(ds$test)   

    audience.ZS =  bas.lm(audience_score ~ .,  
                          data = df.train,
                          prior = "ZS-null",
                          modelprior = uniform(),
                          method = "MCMC", 
                          MCMC.iterations = 10^6)    

    predict.value <- ds$test[2,]
    pred.A <- predict(audience.ZS, newdata = predict.value, estimator="BMA")

    # Error in percent  
    Error_in_percent.A = round(100 - (round(pred.A$Ybma, 2) * 100 / predict.value$audience_score), 2)

    predict.value <- ds$test[34,]
    pred.B <- predict(audience.ZS, newdata = predict.value, estimator="BMA")

    # Error in percent  
    Error_in_percent.B = round(100 - (round(pred.B$Ybma, 2) * 100 / predict.value$audience_score), 2)
    
    # ===============================================================================================
    # Sully 
    # ===============================================================================================
   
    audience.ZS =  bas.lm(audience_score ~ .,  
                          data = data.final,
                          prior = "ZS-null",
                          modelprior = uniform(),
                          method = "MCMC", 
                          MCMC.iterations = 10^6)    
    
    predict.value <- data.frame( imdb_rating = 7.6, critics_score = 74, audience_score = 80 )
    predict.sully <- predict(audience.ZS, newdata = predict.value, estimator="BMA")
    predict.sully$Ybma

```

To verify the model we initially separate the dataset into a training and a test dataset. We train the model with the training data and afterwards use some test sets to verify the defined model.

The predicted value for Sully is ~80, which might be a realistic value, based on the previously validated values. The actual audience score for this particular movie has a lower bound of approximately 60.42 and a higher bound of approximately 100.03.

* * *

## Part 6: Conclusion
The predictive model presented here is used to predict the audience scores for a movie. Using Bayesian model average many models can be constructed to perform better predictions. The proposed linear model shows a 'fairly good' prediction rate, but it should be noted that the model is based on a very small sample. Fact is that imdb_rating has the highest posterior probability, and that basically all of the newly created features were useless to support a better prediction. Creating a model, which has a high predictive power is not so easy to reach. Using Bayes for better prediction is only one part of the game. It might be beneficial to gather more data or try to extend the feature engineering part, which means to creating new meaningful features from existing or gather data for new features. I doubt that a feature as 'summer_season' or if a movie has an mpaa_rating - R is supporting the prediction.

I have my personal view on how I would rely on other features to evaluate a audience score (e.g. genre, actors etc.). What I figured as well is, that Lower bound audience_scores show a much higher difference. Therefore we might think about a second or higher order polynomial model. But we still need be carefull to find a decent balance between over-, and underfitting the model.

